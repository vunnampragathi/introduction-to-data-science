{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ed88ef",
   "metadata": {},
   "source": [
    "## week 9: removing punctuation and stop words from a corpus\n",
    "#### Assignment Content\n",
    "I provide a .zip containing .txt and .docx files​\n",
    "\n",
    "After extracting the .zip contents, use \"glob\" module to get the file names in a list\n",
    "\n",
    "For each file, remove punctuation and stop words​\n",
    "\n",
    "Produce a single .dat file containing the name of each file in quotes, a colon, then a list of words separated by commas​. The list of words per file should be unique for that file. Do not include URLs or phone numbers. Words should be made lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62b199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required modules\n",
    "import pandas as p\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c9d2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named explicitly:\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\n",
      "\n",
      "Named with wildcard *:\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\52256-0.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\53031-0.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\58108-0.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\blind_text.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\dr_yawn.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\how_rubber_goods_are_made.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\most_boring_ever.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\most_boring_part2.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\pg12814.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\pg14895.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\pg43994.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\random_text.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\smiley_the_bunny.txt\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\week_10_document1.docx\n",
      "C:/Users/pragathi/Downloads/week_10_txt_and_docx\\week_10_document2.docx\n"
     ]
    }
   ],
   "source": [
    "print('Named explicitly:')\n",
    "for name in glob.glob('C:/Users/pragathi/Downloads/week_10_txt_and_docx',recursive= True):\n",
    "    print(name)\n",
    "  \n",
    "# Using '*' pattern \n",
    "print('\\nNamed with wildcard *:')\n",
    "dataset=glob.glob('C:/Users/pragathi/Downloads/week_10_txt_and_docx/*')\n",
    "for name in dataset:\n",
    "    print(name)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3702e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pragathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pragathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    " \n",
    "nltk.download('punkt')\n",
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d4e49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75c97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82106813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\ufeff' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16616/1283250789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetdefaultencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\ufeff' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Creating an output file\n",
    "x = open('output.dat','w')\n",
    "\n",
    "for y in range(len(dataset)):\n",
    "    # Loading and reading the file\n",
    "    file = open(dataset[y],'rt', encoding='utf-8', errors='ignore')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Convert to lower case\n",
    "    data = data.lower()     #https://stackoverflow.com/questions/14067267/lower-case-from-a-text-file\n",
    "    \n",
    "    #https://thispointer.com/python-remove-all-numbers-from-string/\n",
    "    #to remove urls \n",
    "    data = re.sub(r'^([\\+][0-9]{1,3}([ \\.\\-])?)?([\\(]{1}[0-9]{3}[\\)])?([0-9A-Z \\.\\-]{1,32})((x|ext|extension)?[0-9]{1,4}?)$','',data)\n",
    "    data = re.sub(r'^[a-zA-Z0-9\\-\\.]+\\.(com|org|net|mil|edu|COM|ORG|NET|MIL|EDU)$','',data)\n",
    "    \n",
    "    # removing stop words and punctuation\n",
    "    tokens = word_tokenize(data)\n",
    "    cleaned_tokens = [token for token in tokens if token not in stopwords\n",
    "                      and token not in punctuation]      #https://www.analyticssteps.com/blogs/nltk-python-tutorial-beginners\n",
    "    len(cleaned_tokens)\n",
    "\n",
    "\n",
    "    # Writing to a file\n",
    "    x.write(dataset[y])\n",
    "    x.write(': ')\n",
    "    for i in range(len(cleaned_tokens)):\n",
    "        x.write(cleaned_tokens[i]).encode(sys.getdefaultencoding(), 'replace')\n",
    "        x.write(',')\n",
    "    x.write('\\n')\n",
    "\n",
    "# Closing the file\n",
    "x.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61973569",
   "metadata": {},
   "source": [
    "# refernces:\n",
    "printing file names using glob:\n",
    "https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/\n",
    "to remove urls:\n",
    "https://www.codegrepper.com/code-examples/python/how+to+remove+urls+from+text+python\n",
    "to covert to lower case:\n",
    "https://stackoverflow.com/questions/14067267/lower-case-from-a-text-file\n",
    "to remove numbers and urls:\n",
    "https://thispointer.com/python-remove-all-numbers-from-string/\n",
    "to remove stop words and punctuation:\n",
    "https://www.analyticssteps.com/blogs/nltk-python-tutorial-beginners\n",
    "file creation:https://www.pythontutorial.net/python-basics/python-create-text-file/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
